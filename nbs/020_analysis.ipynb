{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">fastai Learner extensions useful to perform prediction analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import inspect\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from fastai.interpret import *\n",
    "from fastai.learner import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tsai.data.core import *\n",
    "from tsai.data.preprocessing import *\n",
    "from tsai.imports import *\n",
    "from tsai.inference import *\n",
    "from tsai.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "@delegates(subplots)\n",
    "def show_probas(self:Learner, figsize=(6,6), ds_idx=1, dl=None, one_batch=False, max_n=None, **kwargs):\n",
    "    recorder = copy(self.recorder) # This is to avoid loss of recorded values while generating preds\n",
    "    if dl is None: dl = self.dls[ds_idx]\n",
    "    if one_batch: dl = [dl.one_batch()]\n",
    "    probas, targets = self.get_preds(dl=dl)\n",
    "    if probas.ndim == 2 and probas.min() < 0 or probas.max() > 1: probas = nn.Softmax(-1)(probas)\n",
    "    if not isinstance(targets[0].item(), Integral): return\n",
    "    targets = targets.flatten()\n",
    "    if max_n is not None:\n",
    "        idxs = random_choice(len(probas), max_n, False)\n",
    "        probas, targets = probas[idxs], targets[idxs]\n",
    "    if isinstance(probas, torch.Tensor): probas = probas.detach().cpu().numpy()\n",
    "    if isinstance(targets, torch.Tensor): targets = targets.detach().cpu().numpy()\n",
    "    fig = plt.figure(figsize=figsize, **kwargs)\n",
    "    classes = np.unique(targets)\n",
    "    nclasses = len(classes)\n",
    "    vals = np.linspace(.5, .5 + nclasses - 1, nclasses)[::-1]\n",
    "    plt.vlines(.5, min(vals) - 1, max(vals), color='black', linewidth=.5)\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "    color = [cm(1.* c/nclasses) for c in range(1, nclasses + 1)][::-1]\n",
    "    # class_probas = np.array([probas[i,t] for i,t in enumerate(targets)])\n",
    "    class_probas = np.array([probas[i][t] for i,t in enumerate(targets)])\n",
    "    for i, c in enumerate(classes):\n",
    "        plt.scatter(class_probas[targets == c] if nclasses > 2 or i > 0 else 1 - class_probas[targets == c],\n",
    "                    targets[targets == c] + .5 * (np.random.rand((targets == c).sum()) - .5), color=color[i], edgecolor='black', alpha=.2, s=100)\n",
    "        if nclasses > 2: plt.vlines((targets == c).mean(), i - .5, i + .5, color='r', linewidth=.5)\n",
    "    plt.hlines(vals, 0, 1)\n",
    "    plt.ylim(min(vals) - 1, max(vals))\n",
    "    plt.xlim(0,1)\n",
    "    plt.xticks(np.linspace(0,1,11), fontsize=12)\n",
    "    plt.yticks(classes, [self.dls.vocab[x] for x in classes], fontsize=12)\n",
    "    plt.title('Predicted proba per true class' if nclasses > 2 else 'Predicted class 1 proba per true class', fontsize=14)\n",
    "    plt.xlabel('Probability', fontsize=12)\n",
    "    plt.ylabel('True class', fontsize=12)\n",
    "    plt.grid(axis='x', color='gainsboro', linewidth=.2)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()\n",
    "    self.recorder = recorder\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def plot_confusion_matrix(self:Learner, ds_idx=1, dl=None, thr=.5, normalize=False, title='Confusion matrix', cmap=\"Blues\", norm_dec=2, figsize=(5,5),\n",
    "                          title_fontsize=12, fontsize=10, plot_txt=True, **kwargs):\n",
    "        \"Plot the confusion matrix, with `title` and using `cmap`.\"\n",
    "        # This function is mainly copied from the sklearn docs\n",
    "        if dl is None: dl = self.dls[ds_idx]\n",
    "        assert dl.cat\n",
    "        if dl.c == 2: # binary classification\n",
    "            probas, preds = self.get_preds(dl=dl)\n",
    "            y_pred = (probas[:, 1] > thr).numpy().astype(int)\n",
    "            y_test = preds.numpy()\n",
    "            if normalize: skm_normalize = 'true'\n",
    "            else: skm_normalize = None\n",
    "            cm = skm.confusion_matrix(y_test, y_pred, normalize=skm_normalize)\n",
    "        else: \n",
    "            cm = ClassificationInterpretation.from_learner(self).confusion_matrix()\n",
    "\n",
    "        if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fig = plt.figure(figsize=figsize, **kwargs)\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        if self.dls.c == 2:\n",
    "            plt.title(f\"{title} (threshold: {thr})\", fontsize=title_fontsize)\n",
    "        else: \n",
    "            plt.title(title, fontsize=title_fontsize)\n",
    "        tick_marks = np.arange(len(self.dls.vocab))\n",
    "        plt.xticks(tick_marks, self.dls.vocab, rotation=90, fontsize=fontsize)\n",
    "        plt.yticks(tick_marks, self.dls.vocab, rotation=0, fontsize=fontsize)\n",
    "\n",
    "        if plot_txt:\n",
    "            thresh = cm.max() / 2.\n",
    "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                coeff = f'{cm[i, j]:.{norm_dec}f}' if normalize else f'{cm[i, j]}'\n",
    "                plt.text(j, i, coeff, horizontalalignment=\"center\", verticalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=fontsize)\n",
    "\n",
    "        ax = fig.gca()\n",
    "        ax.set_ylim(len(self.dls.vocab)-.5,-.5)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('Actual', fontsize=fontsize)\n",
    "        plt.xlabel('Predicted', fontsize=fontsize)\n",
    "        plt.grid(False)\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def top_losses(self:Learner,\n",
    "    X, # array-like object representing the independent variables\n",
    "    y, # array-like object representing the target\n",
    "    k:int=9, # Optional. #items to plot\n",
    "    largest=True, # Flag to show largest or smallest losses\n",
    "    bs:int=64, # batch size\n",
    "    ):\n",
    "    *_, losses = self.get_X_preds(X, y, bs=bs, with_loss=True)\n",
    "    top_losses, idxs = losses.topk(ifnone(k, len(losses)), largest=largest)\n",
    "    idxs = idxs.tolist()\n",
    "    return top_losses, idxs\n",
    "\n",
    "@patch\n",
    "def plot_top_losses(self:Learner,\n",
    "    X, # array-like object representing the independent variables\n",
    "    y, # array-like object representing the target\n",
    "    k:int=9, # Optional. #items to plot\n",
    "    largest=True, # Flag to show largest or smallest losses\n",
    "    bs:int=64, # batch size\n",
    "    **kwargs, # show_batch kwargs\n",
    "    ):\n",
    "    *_, losses = self.get_X_preds(X, y, bs=bs, with_loss=True)\n",
    "    idxs = losses.topk(ifnone(k, len(losses)), largest=largest)[1].tolist()\n",
    "    dl = self.dls.valid.new_dl(X[idxs], y=y[idxs], bs=k)\n",
    "    b = dl.one_batch()\n",
    "    dl.show_batch(b, max_n=k, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also introduced 2 methods to help you better understand how important certain features or certain steps are for your model. Both methods use permutation importance. \n",
    "\n",
    "⚠️**The permutation feature or step importance is defined as the decrease in a model score when a single feature or step value is randomly shuffled.**\n",
    "\n",
    "So if you using accuracy (higher is better), the most important features or steps will be those with a *lower* value on the chart (as randomly shuffling them reduces performance). \n",
    "\n",
    "The opposite occurs for metrics like mean squared error (lower is better). In this case, the most important features or steps will be those with a *higher* value on the chart. \n",
    "\n",
    "There are 2 issues with step importance: \n",
    "\n",
    "* there may be many steps and the analysis could take very long\n",
    "* steps will likely have a high autocorrelation\n",
    "\n",
    "For those reasons, we've introduced an argument (n_steps) to group steps. In this way you'll be able to know which part of the time series is the most important. \n",
    "\n",
    "Feature importance has been adapted from https://www.kaggle.com/cdeotte/lstm-feature-importance by Chris Deotte (Kaggle GrandMaster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def feature_importance(self:Learner, \n",
    "    X=None, # array-like object containing the time series. If None, all data in the validation set will be used.\n",
    "    y=None, # array-like object containing the targets. If None, all targets in the validation set will be used.\n",
    "    bs:int=None, # batch size. If None, the default batch size of the dataloader will be used.\n",
    "    partial_n:(int, float)=None, # # (int) or % (float) of used to measure feature importance. If None, all data will be used.\n",
    "    method:str='permutation', # Method used to invalidate feature. Use 'permutation' for shuffling or 'ablation' for setting values to np.nan.\n",
    "    feature_names:list=None, # Optional list of feature names that will be displayed if available. Otherwise var_0, var_1, etc.\n",
    "    sel_classes:(str, list)=None, # classes for which the analysis will be made\n",
    "    key_metric_idx:int=0, # Optional position of the metric used. If None or no metric is available, the loss will be used.\n",
    "    show_chart:bool=True, # Flag to indicate if a chart showing permutation feature importance will be plotted.\n",
    "    figsize:tuple=None, # Size of the chart.\n",
    "    title:str=None, # Optional string that will be used as the chart title. If None 'Permutation Feature Importance'.\n",
    "    return_df:bool=True, # Flag to indicate if the dataframe with feature importance will be returned.\n",
    "    save_df_path:Path=None, # Path where dataframe containing the permutation feature importance results will be saved.\n",
    "    random_state:int=23, # Optional int that controls the shuffling applied to the data.\n",
    "    verbose:bool=True, # Flag that controls verbosity.\n",
    "    ):\n",
    "    r\"\"\"Calculates feature importance as the drop in the model's validation loss or metric when a feature value is randomly shuffled\"\"\"\n",
    "    \n",
    "    assert method in ['permutation', 'ablation']\n",
    "\n",
    "    # X, y\n",
    "    if X is None:\n",
    "        X = self.dls.train.dataset.tls[0].items\n",
    "        if hasattr(self.dls.train.dataset.tls[0], '_splits'): X = X[self.dls.train.dataset.tls[0]._splits]\n",
    "    if y is None:\n",
    "        y = self.dls.train.dataset.tls[1].items\n",
    "    if partial_n is not None:\n",
    "        _, rand_idxs, *_ = train_test_split(np.arange(len(y)), y, test_size=partial_n, random_state=random_state, stratify=y)\n",
    "        X = X.oindex[rand_idxs] if hasattr(X, 'oindex') else X[rand_idxs]\n",
    "        y = y.oindex[rand_idxs] if hasattr(y, 'oindex') else y[rand_idxs]\n",
    "    else: \n",
    "        X, y = X[:], y[:]\n",
    "    if sel_classes is not None:\n",
    "        filt = np.isin(y, listify(sel_classes))\n",
    "        X, y = X[filt], y[filt]\n",
    "    pv(f'X.shape: {X.shape}', verbose)\n",
    "    pv(f'y.shape: {y.shape}', verbose)\n",
    "    if bs is None:\n",
    "        bs = self.dls.valid.bs\n",
    "\n",
    "    # Metrics\n",
    "    metrics = [mn for mn in self.recorder.metric_names if mn not in ['epoch', 'train_loss', 'valid_loss', 'time']]\n",
    "    if len(metrics) == 0 or key_metric_idx is None:\n",
    "        metric_name = self.loss_func.__class__.__name__\n",
    "        key_metric_idx = None\n",
    "    else:\n",
    "        metric_name = metrics[key_metric_idx]\n",
    "        metric = self.recorder.metrics[key_metric_idx].func\n",
    "        if \"sklearn\" in inspect.getmodule(metric).__name__:\n",
    "            sklearn_metric = True\n",
    "        else:\n",
    "            sklearn_metric = False\n",
    "    metric_name = metric_name.replace(\"train_\", \"\").replace(\"valid_\", \"\")\n",
    "    pv(f'Selected metric: {metric_name}', verbose)\n",
    "\n",
    "    # Selected vars & feature names\n",
    "    sel_vars = not(isinstance(self.dls.sel_vars, slice) and self.dls.sel_vars == slice(None, None, None))\n",
    "    if feature_names is None:\n",
    "        feature_names = L([f\"var_{i}\" for i in range(X.shape[1])])\n",
    "        if sel_vars:\n",
    "            feature_names = feature_names[self.dls.sel_vars]\n",
    "    else:\n",
    "        feature_names = listify(feature_names)\n",
    "\n",
    "    if sel_vars:\n",
    "        assert len(feature_names) == len(self.dls.sel_vars)\n",
    "    else:\n",
    "        assert len(feature_names) == X.shape[1]\n",
    "    sel_var_idxs = L(np.arange(X.shape[1]).tolist())\n",
    "    if sel_vars:\n",
    "        sel_var_idxs = sel_var_idxs[self.dls.sel_vars]\n",
    "    assert len(feature_names) == len(sel_var_idxs)\n",
    "    g = list(zip(np.arange(len(sel_var_idxs)+2), [0] + sel_var_idxs))\n",
    "    \n",
    "    # Loop\n",
    "    COLS = ['BASELINE'] + list(feature_names)\n",
    "    results = []\n",
    "    pv(f'Computing feature importance ({method} method)...', verbose)\n",
    "    try:\n",
    "        if method == 'ablation':\n",
    "            fs = self.dls.valid.after_batch.fs\n",
    "            self.dls.valid.after_batch.fs = fs + [TSNan2Value()]\n",
    "        for i,k in progress_bar(g):\n",
    "            if i > 0:\n",
    "                if k not in sel_var_idxs: continue\n",
    "                save_feat = X[:, k].copy()\n",
    "                if method == 'permutation':\n",
    "                    # shuffle along samples & steps\n",
    "                    X[:, k] = random_shuffle(X[:, k].flatten(), random_state=random_state).reshape(X[:, k].shape)\n",
    "                elif method == 'ablation':\n",
    "                    X[:, k] = np.nan\n",
    "            if key_metric_idx is None:\n",
    "                value = self.get_X_preds(X, y, with_loss=True, with_decoded=False, bs=bs)[-1].mean().item()\n",
    "            else:\n",
    "                output = self.get_X_preds(X, y, with_decoded=False, bs=bs)\n",
    "                if self.dls.c == 2:\n",
    "                    try: \n",
    "                        if sklearn_metric:\n",
    "                            value = metric(output[1], output[0][:, 1]).item()\n",
    "                        else:\n",
    "                            value = metric(output[0][:, 1], output[1]).item()\n",
    "                    except: \n",
    "                        if sklearn_metric:\n",
    "                            value = metric(output[1], output[0]).item()\n",
    "                        else:\n",
    "                            value = metric(output[0], output[1]).item()\n",
    "                else:\n",
    "                    if sklearn_metric:\n",
    "                        value = metric(output[1], output[0]).item()\n",
    "                    else:\n",
    "                        value = metric(output[0], output[1]).item()\n",
    "                del output\n",
    "            pv(f\"{k:3} feature: {COLS[i]:20} {metric_name}: {value:8.6f}\", verbose)\n",
    "            results.append([COLS[i], value])\n",
    "            del value; gc.collect()\n",
    "            if i > 0:\n",
    "                X[:, k] = save_feat\n",
    "                del save_feat; gc.collect()\n",
    "        \n",
    "        if method == 'ablation':\n",
    "            self.dls.valid.after_batch.fs = fs\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        if i > 0:\n",
    "            X[:, k] = save_feat\n",
    "            del save_feat; gc.collect()\n",
    "        if method == 'ablation':\n",
    "            self.dls.valid.after_batch.fs = fs\n",
    "\n",
    "    # DataFrame\n",
    "    df = pd.DataFrame(results, columns=[\"Feature\", metric_name])\n",
    "    df[f'{metric_name}_change'] = df[metric_name] - df.loc[0, metric_name]\n",
    "    sign = np.sign(df[f'{metric_name}_change'].mean())\n",
    "    if sign == 0: sign = 1\n",
    "    df[f'{metric_name}_change'] = df[f'{metric_name}_change'] * sign\n",
    "\n",
    "    # Display feature importance\n",
    "    if show_chart:\n",
    "        print()\n",
    "        value_change = df.loc[1:, f'{metric_name}_change'].values\n",
    "        pos_value_change = value_change.copy()\n",
    "        neg_value_change = value_change.copy()\n",
    "        pos_value_change[pos_value_change < 0] = 0\n",
    "        neg_value_change[neg_value_change > 0] = 0\n",
    "        if figsize is None:\n",
    "            figsize=(10, .5*len(value_change))\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.barh(np.arange(len(value_change))[::-1], pos_value_change, color='lime', edgecolor='black')\n",
    "        plt.barh(np.arange(len(value_change))[::-1], neg_value_change, color='red', edgecolor='black')\n",
    "        plt.axvline(0, color='black')\n",
    "        plt.yticks(np.arange(len(value_change))[::-1], df.loc[1:, \"Feature\"].values)\n",
    "        if title is None: title = f'Feature Importance ({method} method)'\n",
    "        plt.title(title, size=16)\n",
    "        text = 'increase' if sign == 1 else 'decrease'\n",
    "        plt.xlabel(f\"{metric_name} {text} when feature is removed\")\n",
    "        plt.ylim((-1,len(value_change)))\n",
    "        plt.show()\n",
    "\n",
    "    # Save feature importance\n",
    "    df = df.sort_values(metric_name, ascending=sign < 0, kind='stable').reset_index(drop=True)\n",
    "    if save_df_path:\n",
    "        if save_df_path.split('.')[-1] != 'csv': save_df_path = f'{save_df_path}.csv'\n",
    "        df.to_csv(f'{save_df_path}', index=False)\n",
    "        pv(f'Feature importance df saved to {save_df_path}', verbose)\n",
    "    if return_df: \n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def step_importance(\n",
    "    self:Learner, \n",
    "    X=None, # array-like object containing the time series. If None, all data in the validation set will be used.\n",
    "    y=None, # array-like object containing the targets. If None, all targets in the validation set will be used.\n",
    "    bs:int=None, # batch size used to compute predictions. If None, the batch size used in the validation set will be used.\n",
    "    partial_n:(int, float)=None, # # (int) or % (float) of used to measure feature importance. If None, all data will be used.\n",
    "    method:str='permutation', # Method used to invalidate feature. Use 'permutation' for shuffling or 'ablation' for setting values to np.nan.\n",
    "    step_names:list=None, # Optional list of step names that will be displayed if available. Otherwise 0, 1, 2, etc.\n",
    "    sel_classes:(str, list)=None, # classes for which the analysis will be made\n",
    "    n_steps:int=1, # # of steps that will be analyzed at a time. Default is 1.\n",
    "    key_metric_idx:int=0, # Optional position of the metric used. If None or no metric is available, the loss will be used.\n",
    "    show_chart:bool=True, # Flag to indicate if a chart showing permutation feature importance will be plotted.\n",
    "    figsize:tuple=(10, 5), # Size of the chart.\n",
    "    title:str=None, # Optional string that will be used as the chart title. If None 'Permutation Feature Importance'.\n",
    "    xlabel=None, # Optional string that will be used as the chart xlabel. If None 'steps'.\n",
    "    return_df:bool=True, # Flag to indicate if the dataframe with feature importance will be returned.\n",
    "    save_df_path:Path=None, # Path where dataframe containing the permutation feature importance results will be saved.\n",
    "    random_state:int=23, # Optional int that controls the shuffling applied to the data.\n",
    "    verbose:bool=True, # Flag that controls verbosity.\n",
    "    ):\n",
    "    r\"\"\"Calculates step importance as the drop in the model's validation loss or metric when a step/s value/s is/are randomly shuffled\"\"\"\n",
    "    \n",
    "    assert method in ['permutation', 'ablation']\n",
    "    \n",
    "    # X, y\n",
    "    if X is None:\n",
    "        X = self.dls.train.dataset.tls[0].items\n",
    "        if hasattr(self.dls.train.dataset.tls[0], '_splits'): X = X[self.dls.train.dataset.tls[0]._splits]\n",
    "    if y is None:\n",
    "        y = self.dls.train.dataset.tls[1].items\n",
    "    if partial_n is not None:\n",
    "        _, rand_idxs, *_ = train_test_split(np.arange(len(y)), y, test_size=partial_n, random_state=random_state, stratify=y)\n",
    "        X = X.oindex[rand_idxs] if hasattr(X, 'oindex') else X[rand_idxs]\n",
    "        y = y.oindex[rand_idxs] if hasattr(y, 'oindex') else y[rand_idxs]\n",
    "    else: \n",
    "        X, y = X[:], y[:]\n",
    "    if sel_classes is not None:\n",
    "        filt = np.isin(y, listify(sel_classes))\n",
    "        X, y = X[filt], y[filt]\n",
    "    pv(f'X.shape: {X.shape}', verbose)\n",
    "    pv(f'y.shape: {y.shape}', verbose)\n",
    "    if bs is None:\n",
    "        bs = self.dls.valid.bs\n",
    "\n",
    "    # Metrics\n",
    "    metrics = [mn for mn in self.recorder.metric_names if mn not in ['epoch', 'train_loss', 'valid_loss', 'time']]\n",
    "    if len(metrics) == 0 or key_metric_idx is None:\n",
    "        metric_name = self.loss_func.__class__.__name__\n",
    "        key_metric_idx = None\n",
    "    else:\n",
    "        metric_name = metrics[key_metric_idx]\n",
    "        metric = self.recorder.metrics[key_metric_idx].func\n",
    "        if \"sklearn\" in inspect.getmodule(metric).__name__:\n",
    "            sklearn_metric = True\n",
    "        else:\n",
    "            sklearn_metric = False\n",
    "    metric_name = metric_name.replace(\"train_\", \"\").replace(\"valid_\", \"\")\n",
    "    pv(f'Selected metric: {metric_name}', verbose)\n",
    "    \n",
    "    # Selected steps\n",
    "    sel_step_idxs = L(np.arange(X.shape[-1]).tolist())[self.dls.sel_steps]\n",
    "    if n_steps != 1:\n",
    "        sel_step_idxs = [listify(sel_step_idxs[::-1][n:n+n_steps][::-1]) for n in range(0, len(sel_step_idxs), n_steps)][::-1]     \n",
    "    g = list(zip(np.arange(len(sel_step_idxs)+2), [0] + sel_step_idxs))\n",
    "\n",
    "    # Loop\n",
    "    COLS = ['BASELINE'] + sel_step_idxs\n",
    "    results = []\n",
    "    _step_names = []\n",
    "    pv('Computing step importance...', verbose)\n",
    "    try:\n",
    "        if method == 'ablation':\n",
    "            fs = self.dls.valid.after_batch.fs\n",
    "            self.dls.valid.after_batch.fs = fs + [TSNan2Value()]\n",
    "        for i,k in progress_bar(g):\n",
    "            if i > 0:\n",
    "                if k not in sel_step_idxs: continue\n",
    "                save_feat = X[..., k].copy()\n",
    "                if method == 'permutation':\n",
    "                    # shuffle along samples\n",
    "                    X[..., k] = shuffle_along_axis(X[..., k], axis=0, random_state=random_state)\n",
    "                elif method == 'ablation':\n",
    "                    X[..., k] = np.nan\n",
    "            if key_metric_idx is None:\n",
    "                value = self.get_X_preds(X, y, bs=bs, with_loss=True, with_decoded=False)[-1].mean().item()\n",
    "            else:\n",
    "                output = self.get_X_preds(X, y, bs=bs, with_decoded=False)\n",
    "                if self.dls.c == 2:\n",
    "                    try: \n",
    "                        if sklearn_metric:\n",
    "                            value = metric(output[1], output[0][:, 1]).item()\n",
    "                        else:\n",
    "                            value = metric(output[0][:, 1], output[1]).item()\n",
    "                    except: \n",
    "                        if sklearn_metric:\n",
    "                            value = metric(output[1], output[0]).item()\n",
    "                        else:\n",
    "                            value = metric(output[0], output[1]).item()\n",
    "                else:\n",
    "                    if sklearn_metric:\n",
    "                        value = metric(output[1], output[0]).item()\n",
    "                    else:\n",
    "                        value = metric(output[0], output[1]).item()\n",
    "                del output\n",
    "            \n",
    "            # Step names\n",
    "            if i == 0 or step_names is None:\n",
    "                if i > 0 and n_steps != 1:\n",
    "                    step_name = f\"{str(COLS[i][0])} to {str(COLS[i][-1])}\"\n",
    "                else: step_name = str(COLS[i])\n",
    "            else:\n",
    "                step_name = step_names[i - 1]\n",
    "            if i > 0: _step_names.append(step_name)\n",
    "                \n",
    "            pv(f\"{i:3} step: {step_name:20} {metric_name}: {value:8.6f}\", verbose)\n",
    "            results.append([step_name, value])\n",
    "            del value; gc.collect()\n",
    "            if i > 0:\n",
    "                X[..., k] = save_feat\n",
    "                del save_feat; gc.collect()\n",
    "        \n",
    "        if method == 'ablation':\n",
    "            self.dls.valid.after_batch.fs = fs\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        if i > 0:\n",
    "            X[..., k] = save_feat\n",
    "            del save_feat; gc.collect()\n",
    "        if method == 'ablation':\n",
    "            self.dls.valid.after_batch.fs = fs\n",
    "\n",
    "    # DataFrame\n",
    "    df = pd.DataFrame(results, columns=[\"Step\", metric_name])\n",
    "    df[f'{metric_name}_change'] = df[metric_name] - df.loc[0, metric_name]\n",
    "    sign = np.sign(df[f'{metric_name}_change'].mean())\n",
    "    if sign == 0: sign = 1\n",
    "    df[f'{metric_name}_change'] = df[f'{metric_name}_change'] * sign\n",
    "    \n",
    "    # Display step importance\n",
    "    if show_chart:\n",
    "        print()\n",
    "        value_change = df.loc[1:, f'{metric_name}_change'].values\n",
    "        pos_value_change = value_change.copy()\n",
    "        neg_value_change = value_change.copy()\n",
    "        pos_value_change[pos_value_change < 0] = 0\n",
    "        neg_value_change[neg_value_change > 0] = 0\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(np.arange(len(value_change)), pos_value_change, color='lime', edgecolor='black')\n",
    "        plt.bar(np.arange(len(value_change)), neg_value_change, color='red', edgecolor='black')\n",
    "        plt.axhline(0, color='black')\n",
    "        plt.xticks(np.arange(len(value_change)), _step_names, rotation=90)\n",
    "        if title is None: title = f'Step Importance ({method} method)'\n",
    "        plt.title(title, size=16)\n",
    "        text = 'increase' if sign == 1 else 'decrease'\n",
    "        if xlabel is None: xlabel = 'steps'\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(f\"{metric_name} {text} when removed\")\n",
    "        plt.xlim((-1,len(value_change)))\n",
    "        plt.show()\n",
    "\n",
    "    # Save step importance\n",
    "    df = df.sort_values(metric_name, ascending=sign < 0, kind='stable').reset_index(drop=True)\n",
    "    if save_df_path:\n",
    "        if save_df_path.split('.')[-1] != 'csv': save_df_path = f'{save_df_path}.csv'\n",
    "        df.to_csv(f'{save_df_path}', index=False)\n",
    "        pv(f'Step importance df saved to {save_df_path}', verbose)\n",
    "    if return_df: \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.external import get_UCR_data\n",
    "from tsai.data.preprocessing import TSRobustScale, TSStandardize\n",
    "from tsai.learner import ts_learner\n",
    "from tsai.models.FCNPlus import FCNPlus\n",
    "from tsai.metrics import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has not been possible to download the required files\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m batch_tfms \u001b[38;5;241m=\u001b[39m TSRobustScale()\n\u001b[0;32m      5\u001b[0m batch_tfms \u001b[38;5;241m=\u001b[39m TSStandardize()\n\u001b[1;32m----> 6\u001b[0m dls \u001b[38;5;241m=\u001b[39m \u001b[43mget_ts_dls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_tfms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_tfms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m learn \u001b[38;5;241m=\u001b[39m ts_learner(dls, FCNPlus, metrics\u001b[38;5;241m=\u001b[39maccuracy, train_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit_one_cycle(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\daniel lakey\\onedrive - esa\\documents\\msc\\tsai\\tsai\\data\\core.py:1093\u001b[0m, in \u001b[0;36mget_ts_dls\u001b[1;34m(X, y, splits, sel_vars, sel_steps, tfms, inplace, path, bs, batch_tfms, num_workers, device, shuffle_train, drop_last, weights, partial_n, sampler, sort, **kwargs)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ts_dls\u001b[39m(X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sel_vars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sel_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tfms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1091\u001b[0m                path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, batch_tfms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shuffle_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   1092\u001b[0m                weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partial_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1093\u001b[0m     splits \u001b[38;5;241m=\u001b[39m \u001b[43m_check_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m     create_dir(path, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1095\u001b[0m     dsets \u001b[38;5;241m=\u001b[39m TSDatasets(X, y, splits\u001b[38;5;241m=\u001b[39msplits, sel_vars\u001b[38;5;241m=\u001b[39msel_vars, sel_steps\u001b[38;5;241m=\u001b[39msel_steps, tfms\u001b[38;5;241m=\u001b[39mtfms, inplace\u001b[38;5;241m=\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\users\\daniel lakey\\onedrive - esa\\documents\\msc\\tsai\\tsai\\data\\core.py:1073\u001b[0m, in \u001b[0;36m_check_splits\u001b[1;34m(X, splits)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_splits\u001b[39m(X, splits):\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m splits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m         _dtype \u001b[38;5;241m=\u001b[39m smallest_dtype(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1074\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e6\u001b[39m: \n\u001b[0;32m   1075\u001b[0m             splits \u001b[38;5;241m=\u001b[39m (L(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(X), dtype\u001b[38;5;241m=\u001b[39m_dtype)\u001b[38;5;241m.\u001b[39mtolist()), L())\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "dsid = 'NATOPS'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "tfms  = [None, [TSClassification()]]\n",
    "batch_tfms = TSRobustScale()\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, splits=splits, sel_vars=[0, 3, 5, 8, 10], sel_steps=slice(-30, None), tfms=tfms, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, FCNPlus, metrics=accuracy, train_metrics=True)\n",
    "learn.fit_one_cycle(2)\n",
    "fig1 = learn.plot_metrics()\n",
    "fig2 = learn.show_probas()\n",
    "fig3 = learn.plot_confusion_matrix()\n",
    "learn.plot_top_losses(X[splits[1]], y[splits[1]], largest=True)\n",
    "learn.top_losses(X[splits[1]], y[splits[1]], largest=True)\n",
    "\n",
    "print(fig1, fig2, fig3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.step_importance(n_steps=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may pass an X and y if you want to analyze a particular group of samples: \n",
    "\n",
    "```bash\n",
    "learn.feature_importance(X=X[splits[1]], y=y[splits[1]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a large validation dataset, you may also use the partial_n argument to select a fixed amount of samples (integer) or a percentage of the validation dataset (float):\n",
    "\n",
    "```bash\n",
    "learn.feature_importance(partial_n=.1)\n",
    "```\n",
    "\n",
    "```bash\n",
    "learn.feature_importance(partial_n=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from tsai.export import get_nb_name; nb_name = get_nb_name(locals())\n",
    "from tsai.imports import create_scripts; create_scripts(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
